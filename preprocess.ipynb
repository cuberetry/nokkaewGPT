{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff2547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize.tcc import segment\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157ace18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tk_segment(text):\n",
    "    subwords = segment(text)\n",
    "    \n",
    "    if len(subwords) == 0:\n",
    "        return ''\n",
    "    elif len(subwords) == 1:\n",
    "        return subwords[0]\n",
    "    \n",
    "    text = [subwords[0]]\n",
    "    for word in subwords[1:]:\n",
    "        if word.isdigit() and text[-1].isdigit():\n",
    "            text[-1] += word\n",
    "        else:\n",
    "            text.append(word)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d01e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "# 'list' to return as a list\n",
    "# 'text' to return as a text file\n",
    "output = 'text'\n",
    "\n",
    "if output == 'text':\n",
    "    with open('./data/thairath.json', 'r') as input_file, open('./data/subword_tokenize.txt', 'w') as subwork_tokenize:\n",
    "        data = json.load(input_file)\n",
    "        i = 0\n",
    "        for lines in data:\n",
    "            line = lines[\"content\"] + '<h>' + lines[\"highlight\"] + \"\\n\"\n",
    "            # if i > 3:\n",
    "            #     break\n",
    "            # print(i)\n",
    "            # print(line)\n",
    "            text = tk_segment(line)\n",
    "            subwork_tokenize.write(' '.join(text))\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            i += 1\n",
    "# Have to fix this chunks\n",
    "elif output == 'list':\n",
    "    subwork_tokenize = []\n",
    "    with open('cleaned_data-used-in-wangchanberta/thwiki-cleaned/thwiki.txt', 'r') as input_file:\n",
    "        i = 0\n",
    "        for line in input_file:\n",
    "            if i > 10000:\n",
    "                break\n",
    "            text = tk_segment(line)\n",
    "            subwork_tokenize.extend(text)\n",
    "            i += 1\n",
    "        size = len(subwork_tokenize)\n",
    "        chunk = 100000\n",
    "        \n",
    "        start = 0\n",
    "        for end in range(chunk, size, chunk):\n",
    "            print(subwork_tokenize[start:end])\n",
    "            start = end\n",
    "            # time.sleep(1)\n",
    "\n",
    "        if start < size:\n",
    "            print(subwork_tokenize[start:size])\n",
    "        print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b853182",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/subword_tokenize.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    new_content = content.replace('   ', ' ').replace('< h >', '<h>')\n",
    "    \n",
    "with open('./data/subword_tokenize.txt', 'w') as file:\n",
    "    file.write(new_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
