{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d01e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.util import normalize, isthai\n",
    "from pythainlp.ulmfit import (process_thai, ungroup_emoji, remove_space, rm_useless_spaces,\n",
    "                              rm_useless_newlines, rm_brackets, replace_wrep_post_nonum, replace_rep_nonum, replace_wrep_post_nonum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9f139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_vector\n",
    "model = word_vector.WordVector(model_name=\"thai2fit_wv\").get_model()\n",
    "words = model.index_to_key\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank\n",
    "\n",
    "thai_letters = '‡∏Å‡∏Ç‡∏É‡∏Ñ‡∏Ö‡∏Ü‡∏á‡∏à‡∏â‡∏ä‡∏ã‡∏å‡∏ç‡∏é‡∏è‡∏ê‡∏ë‡∏í‡∏ì‡∏î‡∏ï‡∏ñ‡∏ó‡∏ò‡∏ô‡∏ö‡∏õ‡∏ú‡∏ù‡∏û‡∏ü‡∏†‡∏°‡∏¢‡∏£‡∏§‡∏§‡πÖ‡∏•‡∏¶‡∏¶‡πÖ‡∏ß‡∏®‡∏©‡∏™‡∏´‡∏¨‡∏≠‡∏Æ‡∏∞‡∏±‡∏≤‡∏≥‡∏¥‡∏µ‡∏∂‡∏∑‡∏∏‡∏π‡πÄ‡πÅ‡πÇ‡πÉ‡πÑ‡πá‡πà‡πâ‡πä‡πã‡πå'\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    if word == '‡∏à‡∏∏‡∏á':\n",
    "        return '‡∏à‡∏±‡∏á'\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = thai_letters\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "# Test function\n",
    "# print(correction('‡∏ô‡∏∞‡∏Ñ‡πà‡∏∞‡πÄ‡∏ó‡∏≠'))\n",
    "# print(correction('‡πÄ‡∏ó‡∏≠'))\n",
    "# print(correction('‡∏à‡∏∏‡∏á'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b2c11e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡∏•‡∏≥‡∏ö‡∏≤‡∏≤‡∏Å‡∏•‡∏≥‡∏ö‡∏ô', '‡πÄ‡πÄ‡∏ï‡πà‡∏á‡∏ï‡∏±‡∏ß', '‡πÅ‡∏ï‡πà‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡∏ô‡∏ô‡∏ô‡∏ô', '()', '‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≤‡∏ô‡∏ô‡∏≤‡∏ô‡∏ô‡∏≤‡∏ô', '‡πÄ‡∏Å‡∏≤‡πà‡πÄ‡∏Å‡∏≤‡πâ', 'üòÇü§£üòÉüòÑüòÖ', 'PyThaiNLP', 'amp;', '‡∏à‡∏∏‡∏á', '‡πÄ‡∏ó‡∏≠', '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö', '‡∏ú‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏û‡∏•', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['‡∏•‡∏≥‡∏ö‡∏≤‡∏≤‡∏Å‡∏•‡∏≥‡∏ö‡∏ô',\n",
       " '‡πÄ‡πÄ‡∏ï‡πà‡∏á‡∏ï‡∏±‡∏ß',\n",
       " '‡πÅ‡∏ï‡πà‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡∏ô‡∏ô‡∏ô‡∏ô',\n",
       " '()',\n",
       " '‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≤‡∏ô‡∏ô‡∏≤‡∏ô‡∏ô‡∏≤‡∏ô',\n",
       " '‡πÄ‡∏Å‡∏≤‡πà‡πÄ‡∏Å‡∏≤‡πâ',\n",
       " 'üòÇü§£üòÉüòÑüòÖ',\n",
       " 'PyThaiNLP',\n",
       " 'amp;',\n",
       " '‡∏à‡∏∏‡∏á',\n",
       " '‡πÄ‡∏ó‡∏≠',\n",
       " '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö',\n",
       " '‡∏ú‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏û‡∏•',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pythainlp\n",
    "from pythainlp.tokenize import sent_tokenize\n",
    "text = \"‡∏•‡∏≥‡∏ö‡∏≤‡∏≤‡∏Å‡∏•‡∏≥‡∏ö‡∏ô ‡πÄ‡πÄ‡∏ï‡πà‡∏á‡∏ï‡∏±‡∏ß ‡πÅ‡∏ï‡πà‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡∏ô‡∏ô‡∏ô‡∏ô () ‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≤‡∏ô‡∏ô‡∏≤‡∏ô‡∏ô‡∏≤‡∏ô ‡πÄ‡∏Å‡∏≤‡πà‡πÄ‡∏Å‡∏≤‡πâ üòÇü§£üòÉüòÑüòÖ PyThaiNLP amp;  ‡∏à‡∏∏‡∏á ‡πÄ‡∏ó‡∏≠ ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ú‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏û‡∏•  \"\n",
    "\n",
    "ptext = process_thai(text,\n",
    "                     pre_rules=[normalize,rm_useless_spaces, rm_useless_newlines,#pythainlp.util.normalize\n",
    "                                rm_brackets, replace_rep_nonum],\n",
    "                     post_rules=[replace_wrep_post_nonum,\n",
    "                                 replace_wrep_post_nonum, remove_space, ungroup_emoji]\n",
    "                     )\n",
    "ptext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af1dce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡∏•‡∏≥‡∏ö‡∏≤‡∏Å‡∏•‡∏≥‡∏ö‡∏ô',\n",
       " '‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡∏ß',\n",
       " '‡πÅ‡∏ï‡πà‡∏á',\n",
       " '‡∏ö‡πâ‡∏≤‡∏ô',\n",
       " '‡∏≠‡∏¢‡∏π‡πà',\n",
       " '‡∏ô‡∏≤‡∏ô',\n",
       " '‡πÄ‡∏Å‡πà‡∏≤',\n",
       " '‡πÄ‡∏Å‡πâ‡∏≤',\n",
       " '‡∏à‡∏∏',\n",
       " '‡∏á',\n",
       " '‡πÄ‡∏ó',\n",
       " '‡∏≠',\n",
       " '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ',\n",
       " '‡∏Ñ‡∏£‡∏±‡∏ö',\n",
       " '‡∏ú‡∏°',\n",
       " '‡∏ä‡∏∑‡πà‡∏≠',\n",
       " '‡πÄ‡∏≠‡∏Å',\n",
       " '‡∏û‡∏•']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptext = [word for word in ptext if isthai(word) and word not in ['xxrep', 'xxwrep']]\n",
    "ptext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e5f485f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡∏•‡∏≥‡∏ö‡∏≤‡∏Å‡∏•‡∏≥‡∏ö‡∏ô',\n",
       " '‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡∏ß',\n",
       " '‡πÅ‡∏ï‡πà‡∏á',\n",
       " '‡∏ö‡πâ‡∏≤‡∏ô',\n",
       " '‡∏≠‡∏¢‡∏π‡πà',\n",
       " '‡∏ô‡∏≤‡∏ô',\n",
       " '‡πÄ‡∏Å‡πà‡∏≤',\n",
       " '‡πÄ‡∏Å‡πâ‡∏≤',\n",
       " '‡∏à‡∏∏‡∏á',\n",
       " '‡πÄ‡∏ó‡∏≠',\n",
       " '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ',\n",
       " '‡∏Ñ‡∏£‡∏±‡∏ö',\n",
       " '‡∏ú‡∏°',\n",
       " '‡∏ä‡∏∑‡πà‡∏≠',\n",
       " '‡πÄ‡∏≠‡∏Å',\n",
       " '‡∏û‡∏•']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine(my_list):\n",
    "    out = []\n",
    "    for i in range(len(my_list)):\n",
    "        if len(my_list[i]) == 1 and i > 0:\n",
    "            out[-1] += my_list[i]\n",
    "        else:\n",
    "            out.append(my_list[i])\n",
    "    return out\n",
    "\n",
    "ptext = combine(ptext)\n",
    "ptext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "518618b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏•‡∏≥‡∏ö‡∏≤‡∏Å‡∏•‡∏≥‡∏ö‡∏ô‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏ï‡πà‡∏á‡∏ö‡πâ‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏Å‡πâ‡∏≤‡∏à‡∏±‡∏á‡πÄ‡∏ò‡∏≠‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏û‡∏•\n"
     ]
    }
   ],
   "source": [
    "ptext = [correction(i) for i in ptext]\n",
    "ptext = ''.join(ptext)\n",
    "\n",
    "print(ptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b56a9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö', '‡∏ú‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏û‡∏•', '‡∏ú‡∏°‡∏´‡∏¥‡∏ß‡∏à‡∏∏‡∏á']\n"
     ]
    }
   ],
   "source": [
    "text = \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ú‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏û‡∏• ‡∏ú‡∏°‡∏´‡∏¥‡∏ß‡∏à‡∏∏‡∏á\"\n",
    "sentences = sent_tokenize(text, engine=\"whitespace\")\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1ba4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect(text):\n",
    "    # Preprocessing\n",
    "    ptext = process_thai(text, pre_rules=[normalize, rm_useless_spaces, rm_useless_newlines, rm_brackets, replace_rep_nonum], \n",
    "                         post_rules=[replace_wrep_post_nonum, replace_wrep_post_nonum, remove_space, ungroup_emoji])\n",
    "    \n",
    "    # Filter out non-Thai and special tokens\n",
    "    ptext = [word for word in ptext if isthai(word) and word not in ['xxrep', 'xxwrep']]\n",
    "    \n",
    "    # Merge single-character tokens with previous token\n",
    "    out = [ptext[0]]\n",
    "    for word in ptext[1:]:\n",
    "        if len(word) == 1 and isthai(out[-1]):\n",
    "            out[-1] += word\n",
    "        else:\n",
    "            out.append(word)\n",
    "    ptext = out\n",
    "    \n",
    "    # Apply spelling correction\n",
    "    ptext = [correction(i) for i in ptext]\n",
    "    \n",
    "    # Combine into a single string\n",
    "    ptext = ''.join(ptext)\n",
    "    \n",
    "    return ptext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc3594dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = [autocorrect(i) for i in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6305762d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö', '‡∏ú‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏û‡∏•', '‡∏ú‡∏°‡∏´‡∏¥‡∏ß‡∏à‡∏±‡∏á']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
